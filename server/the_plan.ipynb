{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mongoDb  is full:\n",
    "```\n",
    " * more than 900k urls to_crawl\n",
    " * >160K crawled_urls\n",
    " * (500MB) Mongo free\n",
    "```\n",
    "#### Solution:\n",
    "* only keep 100k-200K to_crawls in mongo at a time\n",
    "* remove crawled_urls\n",
    "* Avoid crawling data urls\n",
    "\n",
    "get all to_crawl (920054)   | Save to csv\n",
    "get all crawled  (160842)   | Save to csv\n",
    "\n",
    "\n",
    "\n",
    "* make scrapy_engine_spider to push to \"to_crawl?\" instead of \"to_crawl\"\n",
    "* avoid updating \"crawled\" from scrapy_spider to mongo\n",
    "\n",
    "* Get Urls from \"to_crawl?\"\n",
    "    [ ] Store Crawled urls in bloom function.\n",
    "    [ ] bloom filter to check urls in \"to_crawl?\" does not exist in urls_crawled\n",
    "    [X] store to csv/sqlite (avoid duplicate): csv files: \"to_crawl\", \"crawled\" or sqlite_tables\n",
    "    [X] Delete to_crawls and crawled from mongo\n",
    "\n",
    "    if len(to_crawl_in_mongo < 100000):\n",
    "        [ ] shuffle(to_crawl_urls)\n",
    "        [X] add 100k to_crawl urls from local to mongo\n",
    "        [X] delete all attempted mongo inserts.\n",
    "        [ ] delete only successful mongo insert and store unsuccessful ones somewhere\n",
    "\n",
    "update to_crawl and \"crawled\" using crawled_data\n",
    "[X] for data in crawled_data:\n",
    "        if data['parent_url'] in to_crawl:\n",
    "            remove data['parent_url'] from to_crawl\n",
    "        append_to_crawled(data['parent_url'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note:\n",
    "* \"to_crawl?\" -> uploaded by scrapy spider to mongo\n",
    "* \"to_crawl\"  -> actual to_crawl link uploaded to mongo by server\n",
    "* sqlite seems to handle concurrency by itself without throwing errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch Error Data from Mongo and save it to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from mongo import Mongo\n",
    "mongo=Mongo()\n",
    "\n",
    "error_data = mongo.collection.find({'status': 'error'}) # .limit(10)\n",
    "formatted_for_csv = [{\n",
    "        'url': error['url'],\n",
    "        'timestamp': error['timestamp'],\n",
    "        'status': error['status'],\n",
    "        'status_code': error['status_code'] if 'status_code' in error else None,\n",
    "        'error_type': error['error_type']\n",
    "    } for error in error_data]\n",
    "# Append error data to csv\n",
    "csv_file_path = 'error_data.csv'\n",
    "file_exists = os.path.exists(csv_file_path)\n",
    "with open(csv_file_path, 'a') as csvfile:\n",
    "    fieldnames = ['url', 'timestamp', 'status', 'status_code', 'error_type']\n",
    "    csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    if not file_exists:\n",
    "        print(f\"creating csv file: {csv_file_path}\")\n",
    "        # Write header only if file is empty\n",
    "        csv_writer.writeheader()\n",
    "    else:\n",
    "        print(f'csv file: \\\"{csv_file_path}\\\" exists')\n",
    "    csv_writer.writerows(formatted_for_csv)\n",
    "\n",
    "print(f'Saved error_data to {csv_file_path}')\n",
    "\n",
    "# Delete from mongo\n",
    "mongo.collection.delete_many({'url': {'$in': [error['url'] for error in formatted_for_csv]}, 'status': 'error'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_for_csv=[{'url': 'https://www.dainiknepal.com/', 'timestamp': 1714996985.424532, 'status': 'error', 'status_code': 403, 'error_type': 'HttpError'}, {'url': 'https://nepalkhabar.com/', 'timestamp': 1714996985.8666382, 'status': 'error', 'status_code': 403, 'error_type': 'HttpError'}, {'url': 'https://belkotgadhimun.gov.np/faq', 'timestamp': 1715018151.0237563, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://epsnepal.gov.np/documents/service-sector-%e0%a4%ae%e0%a4%be-%e0%a4%b0%e0%a5%8b%e0%a4%b7%e0%a5%8d%e0%a4%9f%e0%a4%b0-%e0%a4%aa%e0%a4%b0%e0%a4%bf%e0%a4%b5%e0%a4%b0%e0%a5%8d%e0%a4%a4%e0%a4%a8-%e0%a4%b8%e0%a4%ae%e0%a5%8d%e0%a4%b5/', 'timestamp': 1715036967.8365157, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://www.bbc.com/nepali/send/u50853473', 'timestamp': 1715078773.2192092, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://www.bbc.com/nepali/undefined', 'timestamp': 1715083194.0109007, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://www.bbc.com/nepali/resources/%5Bhttps:/www.bbc.com/nepali%5D', 'timestamp': 1715092717.9155746, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'http://dohs.gov.np/ne/mohpnep', 'timestamp': 1715097008.6633568, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'http://nhtc.gov.np/index.php/trainingevent/training', 'timestamp': 1715098146.3797266, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}, {'url': 'https://mofaga.gov.np/prov-1', 'timestamp': 1715100428.9299655, 'status': 'error', 'status_code': 404, 'error_type': 'HttpError'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.dainiknepal.com/',\n",
       " 'https://nepalkhabar.com/',\n",
       " 'https://belkotgadhimun.gov.np/faq',\n",
       " 'https://epsnepal.gov.np/documents/service-sector-%e0%a4%ae%e0%a4%be-%e0%a4%b0%e0%a5%8b%e0%a4%b7%e0%a5%8d%e0%a4%9f%e0%a4%b0-%e0%a4%aa%e0%a4%b0%e0%a4%bf%e0%a4%b5%e0%a4%b0%e0%a5%8d%e0%a4%a4%e0%a4%a8-%e0%a4%b8%e0%a4%ae%e0%a5%8d%e0%a4%b5/',\n",
       " 'https://www.bbc.com/nepali/send/u50853473',\n",
       " 'https://www.bbc.com/nepali/undefined',\n",
       " 'https://www.bbc.com/nepali/resources/%5Bhttps:/www.bbc.com/nepali%5D',\n",
       " 'http://dohs.gov.np/ne/mohpnep',\n",
       " 'http://nhtc.gov.np/index.php/trainingevent/training',\n",
       " 'https://mofaga.gov.np/prov-1']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# Get 10 data urls from to_crawl.\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'}).limit(10)\n",
    "\n",
    "# get urls starting with 'data:'\n",
    "data_urls = mongo.collection.find({\"url\": {\"$regex\": \"^data:\"}})\n",
    "list(data_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "920054"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongo.collection.count_documents({\"status\": 'to_crawl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Cursor' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m db \u001b[38;5;241m=\u001b[39m Mongo()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count Entries with status=\"to_crawl\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto_crawl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Cursor' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "db = Mongo()\n",
    "# Count Entries with status=\"to_crawl\"\n",
    "db.collection.find({\"status\": \"to_crawl\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all urls from mongo and save to sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "# get all to_crawl urls 50000 at a time\n",
    "# Initialize skip to 0\n",
    "skip = 0\n",
    "bulk_size = 50000\n",
    "\n",
    "the_to_crawl_urls = []\n",
    "while True:\n",
    "    # Get the next 50000 documents\n",
    "    to_crawl_urls = mongo.collection.find({\"status\": 'to_crawl'}).skip(skip).limit(bulk_size)\n",
    "    # Convert the cursor to a list\n",
    "    crawled_urls_list = list(to_crawl_urls)\n",
    "    # If the list is empty, break the loop\n",
    "    if not crawled_urls_list:\n",
    "        break\n",
    "    # Process the documents\n",
    "    the_to_crawl_urls.extend([(url['url'], url['timestamp']) for url in crawled_urls_list])\n",
    "    # Increase skip by 50000\n",
    "    skip += bulk_size\n",
    "    print(skip)\n",
    "\n",
    "len(the_to_crawl_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"to_crawl\", the_to_crawl_urls)\n",
    "\n",
    "url_db.count_entries(\"to_crawl\")\n",
    "\n",
    "# # Get all the urls from the database\n",
    "# urls = url_db.fetch('to_crawl', 10)\n",
    "# urls\n",
    "\n",
    "# delete from mongo\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "# get all crawled urls 50000 at a time\n",
    "# Initialize skip to 0\n",
    "skip = 0\n",
    "bulk_size = 50000\n",
    "\n",
    "the_crawled_urls = []\n",
    "while True:\n",
    "    # Get the next 50000 documents\n",
    "    crawled_urls = mongo.collection.find({\"status\": 'crawled'}).skip(skip).limit(bulk_size)\n",
    "    # Convert the cursor to a list\n",
    "    crawled_urls_list = list(crawled_urls)\n",
    "    # If the list is empty, break the loop\n",
    "    if not crawled_urls_list:\n",
    "        break\n",
    "    # Process the documents\n",
    "    the_crawled_urls.extend([(url['url'], url['timestamp']) for url in crawled_urls_list])\n",
    "    # Increase skip by 50000\n",
    "    skip += bulk_size\n",
    "    print(skip)\n",
    "\n",
    "len(the_crawled_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", the_crawled_urls)\n",
    "\n",
    "\n",
    "import time;start=time.time();a=url_db.fetch('crawled', 100000);print(start-time.time())\n",
    "\n",
    "# # Get all the urls from the database\n",
    "urls = url_db.fetch('crawled', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 3, 'electionId': ObjectId('7fffffff0000000000000405'), 'opTime': {'ts': Timestamp(1716833546, 33), 't': 1029}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716833546, 35), 'signature': {'hash': b\"\\xa7.\\x84\\xd8K\\x97\\xd6\\xbc\\xf5\\x08W'\\x06\\xe1o\\x9b\\xa0\\xaf\\x1cl\", 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1716833546, 33)}, acknowledged=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "\n",
    "sample_url = {'url': 'https://wdww.iana.org/f_img/2013.1/iana-logo-header.svg1', 'status': 'to_crawl?', 'timestamp': '2021-07-01T00:00:00.000Z'}\n",
    "mongo.collection.insert_one(sample_url)\n",
    "\n",
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "to_crawl_urls\n",
    "\n",
    "# Remove multiple url from \"to_crawl?\"\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl?', \"url\": {\"$in\": [url[0] for url in to_crawl_urls]}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_crawl_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m url_db \u001b[38;5;241m=\u001b[39m URLDatabase(db_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murls.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# url_db.exists(\"crawled\", to_crawl_urls[0][0])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Insert the data into the database\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m url_db\u001b[38;5;241m.\u001b[39mbulk_insert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrawled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mto_crawl_urls\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Delete \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# url_db.count_entries(\"crawled\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# # Get all the urls from the database\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# # urls = url_db.fetch_all('test')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_crawl_urls' is not defined"
     ]
    }
   ],
   "source": [
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "to_crawl_urls\n",
    "\n",
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# url_db.exists(\"crawled\", to_crawl_urls[0][0])\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", to_crawl_urls)\n",
    "\n",
    "# Delete \n",
    "# url_db.count_entries(\"crawled\")\n",
    "\n",
    "# url_db.fetch_all(\"crawled\")\n",
    "\n",
    "# # Get all the urls from the database\n",
    "# # urls = url_db.fetch_all('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "* creating a giant thread to avoid concurrency issues\n",
    "'''\n",
    "import sys\n",
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "\n",
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "# to_crawl_urls\n",
    "\n",
    "# Save to sqlite\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", to_crawl_urls)\n",
    "\n",
    "# delete from mongo\n",
    "mongo.collection.delete_many([{'url': url[0], 'status': 'to_crawl?'} for url in to_crawl_urls])\n",
    "\n",
    "\n",
    "if mongo.collection.count_documents({\"status\": 'to_crawl'}) < 100000:\n",
    "    new_to_crawl_urls = url_db.fetch('to_crawl', 100000)\n",
    "    n_failed_to_upload = 0\n",
    "    try:\n",
    "        # insert many\n",
    "        mongo.collection.insert_many([{'url': url[0], 'status': 'to_crawl', 'timestamp': url[1]} for url in new_to_crawl_urls], ordered=False)\n",
    "    except Exception as bwe:\n",
    "        # pass\n",
    "        # Get the details of the operations that failed\n",
    "        failed_ops = bwe.details['writeErrors']\n",
    "        \n",
    "\n",
    "        # Get the documents that failed to insert\n",
    "        failed_docs = [op['op'] for op in failed_ops]\n",
    "\n",
    "        # Get the URLs that failed to insert\n",
    "        urls_failed_to_upload_to_mongo = [(doc['url'], doc['timestamp']) for doc in failed_docs]\n",
    "        n_failed_to_upload = len(urls_failed_to_upload_to_mongo)\n",
    "        # for url in urls_failed_to_upload_to_mongo:\n",
    "        #     logging.error(f\"Failed to upload {url} to MongoDB\")\n",
    "        # success_urls = [url for url in new_to_crawl_urls if url not in urls_failed_to_upload_to_mongo]\n",
    "\n",
    "        # # Delete successful urls from sqlite\n",
    "        # url_db.delete(\"to_crawl\", success_urls)\n",
    "        # # print(f'success_urls:{success_urls}, len:{len(success_urls)}')\n",
    "        # print(failed_urls, len(failed_urls))\n",
    "    # # delete from sqlite\n",
    "    if n_failed_to_upload < 10000:\n",
    "        url_db.delete(\"to_crawl\", new_to_crawl_urls)\n",
    "        # print(n_failed_to_upload)\n",
    "    else:\n",
    "        print(f'failed to upload {n_failed_to_upload} urls to mongo')\n",
    "        # exit the python script\n",
    "        sys.exit(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 0, 'electionId': ObjectId('7fffffff0000000000000405'), 'opTime': {'ts': Timestamp(1716835666, 11), 't': 1029}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716835666, 11), 'signature': {'hash': b'a\\x0cbO\\xfd\"\\xda\\xff0,*\\xbaK&\\xad@\\xba\\xb5\\xe6o', 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1716835666, 11)}, acknowledged=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# Delete all urls with status 'crawled'\n",
    "# mongo.collection.delete_many({\"status\": 'to_crawl'})\n",
    "mongo.collection.delete_many({\"status\": 'crawled'})\n",
    "mongo.collection.delete_many({\"status\": 'test'})\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# crawled_count = mongo.collection.count_documents({\"status\": 'crawled'})\n",
    "to_crawl_spider_count = mongo.collection.count_documents({\"status\": 'to_crawl?'})\n",
    "to_crawl_spider_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "crawled_data_count = mongo.db['crawled_data'].count_documents({})\n",
    "\n",
    "other_data_count = mongo.db['other_data'].count_documents({})\n",
    "print(crawled_data_count)\n",
    "print(other_data_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
