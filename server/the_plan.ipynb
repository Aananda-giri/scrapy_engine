{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mongoDb  is full:\n",
    "```\n",
    " * more than 900k urls to_crawl\n",
    " * >160K crawled_urls\n",
    " * (500MB) Mongo free\n",
    "```\n",
    "#### Solution:\n",
    "* only keep 100k-200K to_crawls in mongo at a time\n",
    "* remove crawled_urls\n",
    "* Avoid crawling data urls\n",
    "\n",
    "get all to_crawl (920054)   | Save to csv\n",
    "get all crawled  (160842)   | Save to csv\n",
    "\n",
    "\n",
    "\n",
    "* make scrapy_engine_spider to push to \"to_crawl?\" instead of \"to_crawl\"\n",
    "* avoid updating \"crawled\" from scrapy_spider to mongo\n",
    "\n",
    "* Get Urls from \"to_crawl?\"\n",
    "    [ ] Store Crawled urls in bloom function.\n",
    "    [ ] bloom filter to check urls in \"to_crawl?\" does not exist in urls_crawled\n",
    "    [X] store to csv/sqlite (avoid duplicate): csv files: \"to_crawl\", \"crawled\" or sqlite_tables\n",
    "    [X] Delete to_crawls and crawled from mongo\n",
    "\n",
    "    if len(to_crawl_in_mongo < 100000):\n",
    "        [ ] shuffle(to_crawl_urls)\n",
    "        [X] add 100k to_crawl urls from local to mongo\n",
    "        [X] delete all attempted mongo inserts.\n",
    "        [ ] delete only successful mongo insert and store unsuccessful ones somewhere\n",
    "\n",
    "update to_crawl and \"crawled\" using crawled_data\n",
    "[X] for data in crawled_data:\n",
    "        if data['parent_url'] in to_crawl:\n",
    "            remove data['parent_url'] from to_crawl\n",
    "        append_to_crawled(data['parent_url'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Note:\n",
    "* \"to_crawl?\" -> uploaded by scrapy spider to mongo\n",
    "* \"to_crawl\"  -> actual to_crawl link uploaded to mongo by server\n",
    "* sqlite seems to handle concurrency by itself without throwing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# Get 10 data urls from to_crawl.\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'}).limit(10)\n",
    "\n",
    "# get urls starting with 'data:'\n",
    "data_urls = mongo.collection.find({\"url\": {\"$regex\": \"^data:\"}})\n",
    "list(data_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "920054"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mongo.collection.count_documents({\"status\": 'to_crawl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Cursor' object has no attribute 'count'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m db \u001b[38;5;241m=\u001b[39m Mongo()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Count Entries with status=\"to_crawl\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstatus\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto_crawl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Cursor' object has no attribute 'count'"
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "db = Mongo()\n",
    "# Count Entries with status=\"to_crawl\"\n",
    "db.collection.find({\"status\": \"to_crawl\"}).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get all urls from mongo and save to sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "# get all to_crawl urls 50000 at a time\n",
    "# Initialize skip to 0\n",
    "skip = 0\n",
    "bulk_size = 50000\n",
    "\n",
    "the_to_crawl_urls = []\n",
    "while True:\n",
    "    # Get the next 50000 documents\n",
    "    to_crawl_urls = mongo.collection.find({\"status\": 'to_crawl'}).skip(skip).limit(bulk_size)\n",
    "    # Convert the cursor to a list\n",
    "    crawled_urls_list = list(to_crawl_urls)\n",
    "    # If the list is empty, break the loop\n",
    "    if not crawled_urls_list:\n",
    "        break\n",
    "    # Process the documents\n",
    "    the_to_crawl_urls.extend([(url['url'], url['timestamp']) for url in crawled_urls_list])\n",
    "    # Increase skip by 50000\n",
    "    skip += bulk_size\n",
    "    print(skip)\n",
    "\n",
    "len(the_to_crawl_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"to_crawl\", the_to_crawl_urls)\n",
    "\n",
    "url_db.count(\"to_crawl\")\n",
    "\n",
    "# # Get all the urls from the database\n",
    "# urls = url_db.fetch('to_crawl', 10)\n",
    "# urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "# get all crawled urls 50000 at a time\n",
    "# Initialize skip to 0\n",
    "skip = 0\n",
    "bulk_size = 50000\n",
    "\n",
    "the_crawled_urls = []\n",
    "while True:\n",
    "    # Get the next 50000 documents\n",
    "    crawled_urls = mongo.collection.find({\"status\": 'crawled'}).skip(skip).limit(bulk_size)\n",
    "    # Convert the cursor to a list\n",
    "    crawled_urls_list = list(crawled_urls)\n",
    "    # If the list is empty, break the loop\n",
    "    if not crawled_urls_list:\n",
    "        break\n",
    "    # Process the documents\n",
    "    the_crawled_urls.extend([(url['url'], url['timestamp']) for url in crawled_urls_list])\n",
    "    # Increase skip by 50000\n",
    "    skip += bulk_size\n",
    "    print(skip)\n",
    "\n",
    "len(the_crawled_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", the_crawled_urls)\n",
    "\n",
    "\n",
    "import time;start=time.time();a=url_db.fetch('crawled', 100000);print(start-time.time())\n",
    "\n",
    "# # Get all the urls from the database\n",
    "urls = url_db.fetch('crawled', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 3, 'electionId': ObjectId('7fffffff0000000000000405'), 'opTime': {'ts': Timestamp(1716833546, 33), 't': 1029}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716833546, 35), 'signature': {'hash': b\"\\xa7.\\x84\\xd8K\\x97\\xd6\\xbc\\xf5\\x08W'\\x06\\xe1o\\x9b\\xa0\\xaf\\x1cl\", 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1716833546, 33)}, acknowledged=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "\n",
    "sample_url = {'url': 'https://wdww.iana.org/f_img/2013.1/iana-logo-header.svg1', 'status': 'to_crawl?', 'timestamp': '2021-07-01T00:00:00.000Z'}\n",
    "mongo.collection.insert_one(sample_url)\n",
    "\n",
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "to_crawl_urls\n",
    "\n",
    "# Remove multiple url from \"to_crawl?\"\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl?', \"url\": {\"$in\": [url[0] for url in to_crawl_urls]}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to_crawl_urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m url_db \u001b[38;5;241m=\u001b[39m URLDatabase(db_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murls.db\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# url_db.exists(\"crawled\", to_crawl_urls[0][0])\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Insert the data into the database\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m url_db\u001b[38;5;241m.\u001b[39mbulk_insert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrawled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mto_crawl_urls\u001b[49m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Delete \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# url_db.count_entries(\"crawled\")\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# # Get all the urls from the database\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# # urls = url_db.fetch_all('test')\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'to_crawl_urls' is not defined"
     ]
    }
   ],
   "source": [
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "to_crawl_urls\n",
    "\n",
    "# Save to sqlite\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "# url_db.exists(\"crawled\", to_crawl_urls[0][0])\n",
    "\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", to_crawl_urls)\n",
    "\n",
    "# Delete \n",
    "# url_db.count_entries(\"crawled\")\n",
    "\n",
    "# url_db.fetch_all(\"crawled\")\n",
    "\n",
    "# # Get all the urls from the database\n",
    "# # urls = url_db.fetch_all('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Final Code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "* creating a giant thread to avoid concurrency issues\n",
    "'''\n",
    "import sys\n",
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "from sqlite_handler import URLDatabase\n",
    "url_db = URLDatabase(db_path=\"urls.db\")\n",
    "\n",
    "\n",
    "# Get all urls from \"to_crawl?\"\n",
    "urls = mongo.collection.find({\"status\": 'to_crawl?'})\n",
    "to_crawl_urls = [(url['url'], url['timestamp']) for url in list(urls)]\n",
    "# to_crawl_urls\n",
    "\n",
    "# Save to sqlite\n",
    "# Insert the data into the database\n",
    "url_db.bulk_insert(\"crawled\", to_crawl_urls)\n",
    "\n",
    "# delete from mongo\n",
    "mongo.collection.delete_many([{'url': url[0], 'status': 'to_crawl?'} for url in to_crawl_urls])\n",
    "\n",
    "\n",
    "if mongo.collection.count_documents({\"status\": 'to_crawl'}) < 100000:\n",
    "    new_to_crawl_urls = url_db.fetch('to_crawl', 100000)\n",
    "    n_failed_to_upload = 0\n",
    "    try:\n",
    "        # insert many\n",
    "        mongo.collection.insert_many([{'url': url[0], 'status': 'to_crawl', 'timestamp': url[1]} for url in new_to_crawl_urls], ordered=False)\n",
    "    except Exception as bwe:\n",
    "        # pass\n",
    "        # Get the details of the operations that failed\n",
    "        failed_ops = bwe.details['writeErrors']\n",
    "        \n",
    "\n",
    "        # Get the documents that failed to insert\n",
    "        failed_docs = [op['op'] for op in failed_ops]\n",
    "\n",
    "        # Get the URLs that failed to insert\n",
    "        urls_failed_to_upload_to_mongo = [(doc['url'], doc['timestamp']) for doc in failed_docs]\n",
    "        n_failed_to_upload = len(urls_failed_to_upload_to_mongo)\n",
    "        # for url in urls_failed_to_upload_to_mongo:\n",
    "        #     logging.error(f\"Failed to upload {url} to MongoDB\")\n",
    "        # success_urls = [url for url in new_to_crawl_urls if url not in urls_failed_to_upload_to_mongo]\n",
    "\n",
    "        # # Delete successful urls from sqlite\n",
    "        # url_db.delete(\"to_crawl\", success_urls)\n",
    "        # # print(f'success_urls:{success_urls}, len:{len(success_urls)}')\n",
    "        # print(failed_urls, len(failed_urls))\n",
    "    # # delete from sqlite\n",
    "    if n_failed_to_upload < 10000:\n",
    "        url_db.delete(\"to_crawl\", new_to_crawl_urls)\n",
    "        # print(n_failed_to_upload)\n",
    "    else:\n",
    "        print(f'failed to upload {n_failed_to_upload} urls to mongo')\n",
    "        # exit the python script\n",
    "        sys.exit(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 0, 'electionId': ObjectId('7fffffff0000000000000405'), 'opTime': {'ts': Timestamp(1716835666, 11), 't': 1029}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716835666, 11), 'signature': {'hash': b'a\\x0cbO\\xfd\"\\xda\\xff0,*\\xbaK&\\xad@\\xba\\xb5\\xe6o', 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1716835666, 11)}, acknowledged=True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# Delete all urls with status 'crawled'\n",
    "# mongo.collection.delete_many({\"status\": 'to_crawl'})\n",
    "mongo.collection.delete_many({\"status\": 'crawled'})\n",
    "mongo.collection.delete_many({\"status\": 'test'})\n",
    "mongo.collection.delete_many({\"status\": 'to_crawl?'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DB stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigurationError",
     "evalue": "The resolution lifetime expired after 21.125 seconds: Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmongo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mongo\n\u001b[0;32m----> 2\u001b[0m mongo \u001b[38;5;241m=\u001b[39m \u001b[43mMongo\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# crawled_count = mongo.collection.count_documents({\"status\": 'crawled'})\u001b[39;00m\n\u001b[1;32m      5\u001b[0m to_crawl_spider_count \u001b[38;5;241m=\u001b[39m mongo\u001b[38;5;241m.\u001b[39mcollection\u001b[38;5;241m.\u001b[39mcount_documents({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto_crawl?\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[0;32m/mnt/resources2/weekly-projects/scrapy_engine/server/mongo.py:17\u001b[0m, in \u001b[0;36mMongo.__init__\u001b[0;34m(self, db_name, collection_name)\u001b[0m\n\u001b[1;32m     13\u001b[0m uri \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmongodb+srv://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmongo_username\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmongo_password\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m@scrapy-engine.cnaygdb.mongodb.net/?retryWrites=true&w=majority&appName=scrapy-engine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# uri = f\"mongodb+srv://{os.environ.get('user_heroku')}:{os.environ.get('pass_heroku')}@cluster0.dgeujbs.mongodb.net/?retryWrites=true&w=majority\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mMongoClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_api\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mServerApi\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Create the database for our example (we will use the same database throughout the tutorial\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdb \u001b[38;5;241m=\u001b[39m client[db_name]\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/mongo_client.py:774\u001b[0m, in \u001b[0;36mMongoClient.__init__\u001b[0;34m(self, host, port, document_class, tz_aware, connect, type_registry, **kwargs)\u001b[0m\n\u001b[1;32m    770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    771\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mvalidate_timeout_or_none_or_zero(\n\u001b[1;32m    772\u001b[0m         keyword_opts\u001b[38;5;241m.\u001b[39mcased_key(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnecttimeoutms\u001b[39m\u001b[38;5;124m\"\u001b[39m), timeout\n\u001b[1;32m    773\u001b[0m     )\n\u001b[0;32m--> 774\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43muri_parser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_uri\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    775\u001b[0m \u001b[43m    \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwarn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconnect_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrv_service_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrv_service_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43msrv_max_hosts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrv_max_hosts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    784\u001b[0m seeds\u001b[38;5;241m.\u001b[39mupdate(res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodelist\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    785\u001b[0m username \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musername\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m username\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/uri_parser.py:547\u001b[0m, in \u001b[0;36mparse_uri\u001b[0;34m(uri, default_port, validate, warn, normalize, connect_timeout, srv_service_name, srv_max_hosts)\u001b[0m\n\u001b[1;32m    545\u001b[0m connect_timeout \u001b[38;5;241m=\u001b[39m connect_timeout \u001b[38;5;129;01mor\u001b[39;00m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnectTimeoutMS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    546\u001b[0m dns_resolver \u001b[38;5;241m=\u001b[39m _SrvResolver(fqdn, connect_timeout, srv_service_name, srv_max_hosts)\n\u001b[0;32m--> 547\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[43mdns_resolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_hosts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    548\u001b[0m dns_options \u001b[38;5;241m=\u001b[39m dns_resolver\u001b[38;5;241m.\u001b[39mget_options()\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dns_options:\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/srv_resolver.py:131\u001b[0m, in \u001b[0;36m_SrvResolver.get_hosts\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_hosts\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m--> 131\u001b[0m     _, nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_srv_response_and_hosts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m nodes\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/srv_resolver.py:111\u001b[0m, in \u001b[0;36m_SrvResolver._get_srv_response_and_hosts\u001b[0;34m(self, encapsulate_errors)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_srv_response_and_hosts\u001b[39m(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m, encapsulate_errors: \u001b[38;5;28mbool\u001b[39m\n\u001b[1;32m    110\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[resolver\u001b[38;5;241m.\u001b[39mAnswer, \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]]:\n\u001b[0;32m--> 111\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolve_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencapsulate_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;66;03m# Construct address tuples\u001b[39;00m\n\u001b[1;32m    114\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    115\u001b[0m         (maybe_decode(res\u001b[38;5;241m.\u001b[39mtarget\u001b[38;5;241m.\u001b[39mto_text(omit_final_dot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)), res\u001b[38;5;241m.\u001b[39mport) \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    116\u001b[0m     ]\n",
      "File \u001b[0;32m~/mambaforge/envs/machine_env/lib/python3.10/site-packages/pymongo/srv_resolver.py:105\u001b[0m, in \u001b[0;36m_SrvResolver._resolve_uri\u001b[0;34m(self, encapsulate_errors)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# Else, raise all errors as ConfigurationError.\u001b[39;00m\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConfigurationError(\u001b[38;5;28mstr\u001b[39m(exc)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mConfigurationError\u001b[0m: The resolution lifetime expired after 21.125 seconds: Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out.; Server Do53:192.168.1.1@53 answered The DNS operation timed out."
     ]
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "# crawled_count = mongo.collection.count_documents({\"status\": 'crawled'})\n",
    "to_crawl_spider_count = mongo.collection.count_documents({\"status\": 'to_crawl?'})\n",
    "to_crawl_spider_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 4565, 'electionId': ObjectId('7fffffff0000000000000405'), 'opTime': {'ts': Timestamp(1716866814, 4581), 't': 1029}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1716866814, 4585), 'signature': {'hash': b'\\x8c\\x88S3\\x07\\xd2\\xa6E\\x94\\xe6\\xbc@\\xfe\\xa8\\xb4$\\x06\\xc9\\x05>', 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1716866814, 4581)}, acknowledged=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crawled_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
