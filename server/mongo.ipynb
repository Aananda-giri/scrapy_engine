{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store to Mongo:\n",
    "* url_to_crawl\n",
    "* url_crawled\n",
    "* url_crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mongo_db_handler import MongoDBHandler\n",
    "import time\n",
    "\n",
    "class Mongo():\n",
    "    def __init__(self):\n",
    "        self.db_handler = MongoDBHandler(collection_name=\"scrapy-engine\", db_name=\"scrapy-engine\")\n",
    "\n",
    "    def append_url_crawled(self, url):\n",
    "        # Add to url_crawled if it does not already exists\n",
    "        if not db_handler.exists(field='url', vlaue=url, collection_name='url_crawled'):\n",
    "            # Check if it does not already exists\n",
    "            db_handler.insert_one({'url':url, 'timestamp':time.time()}, collection_name='url_crawled')\n",
    "        \n",
    "        # Remove from url_crawling\n",
    "        db_handler.delete_one(field='url', value=url, collection_name='url_crawling')        \n",
    "        \n",
    "        # Remove from url_to_crawl\n",
    "        db_handler.delete_one(field='url', value=url, collection_name='url_to_crawl')\n",
    "        \n",
    "    def append_url_crawling(self, url):\n",
    "        # Add to url_crawling\n",
    "        if self.not_crawling_or_not_crawled():\n",
    "            # Check if url has not already been crawled\n",
    "            db_handler.insert_one({'url':url, 'timestamp':time.time()}, collection_name='url_crawling')\n",
    "            \n",
    "            # Remove from url_to_crawl\n",
    "            db_handler.delete_one(field='url', value=url, collection_name='url_to_crawl')\n",
    "            \n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "\n",
    "    def append_url_to_crawl(self, url):\n",
    "        # Add to url_crawling\n",
    "        if self.not_to_crawl_or_not_crawling_or_not_crawled(url):\n",
    "            db_handler.insert_one({'url':url, 'timestamp':time.time()}, collection_name='url_to_crawl')\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def not_crawling_or_not_crawled(self, url):\n",
    "        if not db_handler.exists(field='url', vlaue=url, collection_name='url_crawled'):\n",
    "            if not db_handler.exists(field='url', vlaue=url, collection_name='url_crawling'):\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def not_to_crawl_or_not_crawling_or_not_crawled(self, url):\n",
    "        if not db_handler.exists(field='url', vlaue=url, collection_name='url_crawled'):\n",
    "            if not db_handler.exists(field='url', vlaue=url, collection_name='url_crawling'):\n",
    "                if not db_handler.exists(field='url', vlaue=url, collection_name='url_to_crawl'):\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    def get_expired_url_crawling(self):\n",
    "        # crawling urls expire in every 2 hours\n",
    "        return db_handler.get_items_before_timestamp(timestamp=time.time() - 7200, collection_name='url_crawling')\n",
    "\n",
    "    def fetch_urls_to_crawl(self, number_of_urls_required=10):\n",
    "        # return [json.loads(url) for url in self.redis_client.srandmember('urls_to_crawl_cleaned_set', number_of_new_urls_required)]\n",
    "        \n",
    "        # get urls from to_crawl\n",
    "        urls = db_handler.get_n_items(collection_name='url_to_crawl', n=number_of_urls_required)\n",
    "        \n",
    "        # append them to crawling   -> removes from to_crawl\n",
    "        for url in urls:\n",
    "            self.append_url_crawling(url)\n",
    "        \n",
    "        # return urls\n",
    "        return urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One time operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate initial Start Urls\n",
    "import time\n",
    "from mongo_db_handler import MongoDBHandler\n",
    "db_handler = MongoDBHandler(collection_name=\"scrapy-engine\", db_name=\"scrapy-engine\")\n",
    "\n",
    "start_urls = [\"https://onlinemajdoor.com/\", \"http://nepalipost.com/beta/\", \"https://nepalkhabar.com/\", \"https://www.nepalipaisa.com/\", \"https://topnepalnews.com/\",  \"https://www.dainiknepal.com/\", \"https://www.bbc.com/nepali\"]\n",
    "\n",
    "db_handler.insert_many([{'url':url, 'timestamp':time.time(), 'status':'to_crawl'} for url in start_urls], collection_name='urls-collection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexing by url\n",
    "mongo = Mongo()\n",
    "\n",
    "collection_names = ['url_crawled', 'url_to_crawl', 'url_crawling']\n",
    "for collection_name in collection_names:\n",
    "    mongo.db_handler.delete_all(collection_name=collection_name)\n",
    "    mongo.db_handler.db[collection_name].create_index('url', unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert crawled to to_crawl\n",
    "\n",
    "from mongo import Mongo\n",
    "db=Mongo()\n",
    "db.recover_expired_crawling(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteResult({'n': 419, 'electionId': ObjectId('7fffffff0000000000000400'), 'opTime': {'ts': Timestamp(1714993939, 435), 't': 1024}, 'ok': 1.0, '$clusterTime': {'clusterTime': Timestamp(1714993939, 443), 'signature': {'hash': b'\\xcf\\xc11\\r^(r\\xd3a\\xb0\\xba\\xf2\\xe9e:\\xcb)\\xb3\\xae\\t', 'keyId': 7318892626235621378}}, 'operationTime': Timestamp(1714993939, 435)}, acknowledged=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print all urls with status to_crawl\n",
    "print(list(db.collection.find({'status':'to_crawl'})))\n",
    "\n",
    "\n",
    "# # Delete all (does-not require re-indexing)\n",
    "db.collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('663b0b67cb566c7e819caafc'),\n",
       "  'url': 'https://www.bbc.com/nepali/news-51337730#end-of-twitter-content-2',\n",
       "  'timestamp': 1715145575.5546532,\n",
       "  'status': 'to_crawl'},\n",
       " {'_id': ObjectId('663b0b67cb566c7e819caafd'),\n",
       "  'url': 'https://www.bbc.com/nepali/news-51337730#end-of-twitter-content-3',\n",
       "  'timestamp': 1715145575.7721984,\n",
       "  'status': 'to_crawl'},\n",
       " {'_id': ObjectId('663b0b6b7ee3364c14b6ad72'),\n",
       "  'url': 'https://www.bbc.com/nepali/news-51528594#content',\n",
       "  'timestamp': 1715145579.899758,\n",
       "  'status': 'to_crawl'}]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get 3 urls from to_crawl\n",
    "from mongo import Mongo\n",
    "mongo = Mongo()\n",
    "\n",
    "list(mongo.collection.find({'status':'to_crawl'}).limit(3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mongo import Mongo\n",
    "db = Mongo()\n",
    "error_data = {'url': 'https://nepalkhabar.com/', 'timestamp': 1714993152.5575845, 'status': 'error', 'status_code': 403, 'error_type': 'HttpError'}\n",
    "db.append_error_data(error_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'_id': ObjectId('6638bb23a304c2ebb7f6fe81'),\n",
       "  'url': 'https://nepalkhabar.com/',\n",
       "  'timestamp': 1714993954.888785,\n",
       "  'status': 'error',\n",
       "  'status_code': 403,\n",
       "  'error_type': 'HttpError'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(db.collection.find({'status':'error'}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ioe_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
